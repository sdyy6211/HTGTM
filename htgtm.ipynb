{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from htgtm_model import HTGTM,GNNNet,TransformerBlock\n",
    "from pytorch_tabnet import tab_network\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process each data modality (may take some time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished processing time-series data\n"
     ]
    }
   ],
   "source": [
    "!python ./htgtm_prepare_tabular_data.py\n",
    "!python ./htgtm_prepare_graph_data.py\n",
    "!python ./htgtm_prepare_time-series_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_remapping = pickle.load(open(r'.\\user_id_remapping.pkl','rb'))\n",
    "\n",
    "tabular_data_train = pd.read_csv(r'.\\temp\\processed_tabular_data_train.csv',index_col = 0).sort_index()\n",
    "tabular_data_test = pd.read_csv(r'.\\temp\\processed_tabular_data_test.csv',index_col = 0).sort_index()\n",
    "\n",
    "data_cols = tabular_data_train.columns\n",
    "\n",
    "tabular_data = pd.concat([tabular_data_train, tabular_data_test], axis=0)\n",
    "\n",
    "tabular_data.index = tabular_data.index.map(lambda x: user_id_remapping[x])\n",
    "\n",
    "tabular_data_xgboost = tabular_data.copy()\n",
    "\n",
    "tabular_data = tabular_data.values\n",
    "\n",
    "tabular_data = (tabular_data - tabular_data.mean(axis=0)) / (tabular_data.std(axis=0) + 1e-8)\n",
    "\n",
    "with open(r'.\\temp\\G.pkl','rb') as f:\n",
    "    G = pickle.load(f)\n",
    "\n",
    "graphdata = from_networkx(G)\n",
    "\n",
    "tsdatavalues_full = np.load(r'.\\temp\\datavalues_full_std.npy')\n",
    "tstime_stamps_full = np.load(r'.\\temp\\time_stamps_full_std.npy')\n",
    "tsmasks_full = np.load(r'.\\temp\\masks_full_std.npy')\n",
    "\n",
    "train_index = pickle.load(open(r'.\\train_index.pkl','rb'))\n",
    "val_index = pickle.load(open(r'.\\val_index.pkl','rb'))\n",
    "\n",
    "target = pd.read_csv(r'.\\retention_gt_train.csv',index_col = 0).sort_index()\n",
    "target.index = target.index.map(lambda x: user_id_remapping[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data reweighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sdyy6\\anaconda3\\envs\\py39\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:2640: RuntimeWarning: invalid value encountered in multiply\n",
      "  Lhat = muhat - Shat*mu\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pareto\n",
    "import numpy as np\n",
    "\n",
    "# fit the data to a Pareto distribution\n",
    "params = pareto.fit(target)\n",
    "\n",
    "pdf_fitted = pareto.pdf(target, *params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_d = 8\n",
    "n_a = 8\n",
    "n_steps = 3\n",
    "gamma = 1.3\n",
    "cat_idxs = []\n",
    "cat_dims = [] \n",
    "cat_emb_dims = [] \n",
    "n_independent = 2\n",
    "n_shared = 2\n",
    "epsilon = 1e-15\n",
    "momentum = 0.02\n",
    "lambda_sparse = 1e-3\n",
    "seed = 0\n",
    "clip_value = 1\n",
    "verbose = 1\n",
    "optimizer_fn = torch.optim.Adam\n",
    "optimizer_params = dict(lr=2e-2)\n",
    "mask_type = \"sparsemax\"\n",
    "input_dim = None\n",
    "output_dim = None\n",
    "device_name = \"auto\"\n",
    "n_shared_decoder = 1\n",
    "n_indep_decoder = 1\n",
    "patience=10\n",
    "virtual_batch_size = 128\n",
    "\n",
    "gnnmodel = GNNNet(in_channels=tabular_data.shape[1], embedding_dim=32, out_channels=16, dropout_rate=0.5, number_of_layers=3)\n",
    "\n",
    "tsmodel = TransformerBlock()\n",
    "\n",
    "tabmodel = tab_network.TabNet(tabular_data.shape[1],\n",
    "                           16,\n",
    "                            n_d=n_d,\n",
    "                            n_a=n_a,\n",
    "                            n_steps=n_steps,\n",
    "                            gamma=gamma,\n",
    "                            cat_idxs=cat_idxs,\n",
    "                            cat_dims=cat_dims,\n",
    "                            cat_emb_dim=cat_emb_dims,\n",
    "                            n_independent=n_independent,\n",
    "                            n_shared=n_shared,\n",
    "                            epsilon=epsilon,\n",
    "                            virtual_batch_size=virtual_batch_size,\n",
    "                            momentum=momentum,\n",
    "                            mask_type=mask_type)\n",
    "\n",
    "model = HTGTM(gnnmodel, tsmodel, tabmodel).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple batch_index loader with shuffle = True\n",
    "dataloader_train = torch.utils.data.DataLoader(np.concatenate([\n",
    "                                                                train_index\n",
    "                                                                ]), batch_size=64, shuffle=True)\n",
    "dataloader_total = torch.utils.data.DataLoader(range(\n",
    "                                                     tabular_data_xgboost.shape[0]\n",
    "                                                        ), batch_size=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# define a weighted mse loss\n",
    "def weighted_mse_loss(input, target, weight):\n",
    "    return torch.mean(weight * (input - target) ** 2)\n",
    "\n",
    "loss_fn = weighted_mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the model on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training finished, epoch:  0\n",
      "training finished, epoch:  1\n",
      "training finished, epoch:  2\n",
      "training finished, epoch:  3\n",
      "training finished, epoch:  4\n",
      "training finished, epoch:  5\n",
      "training finished, epoch:  6\n",
      "training finished, epoch:  7\n",
      "training finished, epoch:  8\n",
      "training finished, epoch:  9\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "\n",
    "    if epoch < 5:\n",
    "\n",
    "        loss_weights = pd.DataFrame(np.log1p(10000+1/pdf_fitted)/(np.log1p(10000+1/pdf_fitted).mean()), index = target.index,columns = ['weight'])\n",
    "\n",
    "    else:\n",
    "\n",
    "        loss_weights = pd.DataFrame(np.log1p(100+1/pdf_fitted)/(np.log1p(100+1/pdf_fitted).mean()), index = target.index,columns = ['weight'])\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    for batch_index_train in dataloader_train:\n",
    "\n",
    "        batch_index_train = batch_index_train.long()\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        features = torch.tensor(tabular_data[batch_index_train,:]).float().cuda()\n",
    "\n",
    "        node_ts_feature = torch.tensor(tsdatavalues_full[batch_index_train,:,:]).view(-1,445,4).float().cuda()\n",
    "        tstimestamp = torch.tensor(tstime_stamps_full[batch_index_train,:,:]).long().view(-1,445).cuda()\n",
    "        tsmask = torch.tensor(tsmasks_full[batch_index_train,:,:]).bool().view(-1,445).cuda()\n",
    "\n",
    "        node_features = torch.tensor(tabular_data).float().cuda()\n",
    "        edge_index = graphdata.edge_index.long().cuda()\n",
    "        edge_attr = graphdata.weight.float().cuda()\n",
    "\n",
    "        gt = torch.tensor(target.values[batch_index_train]).float().cuda()\n",
    "        sample_weight = torch.tensor(loss_weights.values[batch_index_train]).float().cuda()\n",
    "\n",
    "        output, M_loss = model(features, node_features, edge_index, edge_attr, batch_index_train, node_ts_feature, tstimestamp, tsmask)\n",
    "\n",
    "        loss = loss_fn(output, gt, sample_weight) - lambda_sparse * M_loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        clip_grad_norm_(model.parameters(), 1)\n",
    "\n",
    "    print('training finished, epoch: ', epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make predictions to all data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nn = []\n",
    "\n",
    "for batch_index_total in dataloader_total:\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        batch_index_total = batch_index_total.long()\n",
    "\n",
    "        features = torch.tensor(tabular_data[batch_index_total,:]).view(-1,17).float().cuda()\n",
    "\n",
    "        node_ts_feature = torch.tensor(tsdatavalues_full[batch_index_total,:,:]).view(-1,445,4).float().cuda()\n",
    "        tstimestamp = torch.tensor(tstime_stamps_full[batch_index_total,:,:]).long().view(-1,445).cuda()\n",
    "        tsmask = torch.tensor(tsmasks_full[batch_index_total,:,:]).bool().view(-1,445).cuda()\n",
    "\n",
    "        node_features = torch.tensor(tabular_data).float().cuda()\n",
    "        edge_index = graphdata.edge_index.long().cuda()\n",
    "        edge_attr = graphdata.weight.float().cuda()\n",
    "\n",
    "        output, M_loss = model(features, node_features, edge_index, edge_attr, batch_index_total, node_ts_feature, tstimestamp, tsmask)\n",
    "\n",
    "        #clip output to larger than 0\n",
    "        output = torch.clamp(output, min=0)\n",
    "\n",
    "        pred_nn.append(output.cpu().numpy())\n",
    "\n",
    "pred_nn = np.concatenate(pred_nn).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tune a xgboost regressor for ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "xtrain = np.concatenate([tabular_data_xgboost.values[train_index,:], pred_nn[train_index]], axis=1)\n",
    "ytrain = target.values[train_index]\n",
    "\n",
    "# subsample the training set to speed up the random search\n",
    "sample_indices = np.random.choice(xtrain.shape[0], size=10000, replace=False)\n",
    "xtrain_sampled = xtrain[sample_indices]\n",
    "ytrain_sampled = ytrain[sample_indices]\n",
    "\n",
    "xval = np.concatenate([tabular_data_xgboost.values[val_index,:], pred_nn[val_index]], axis=1)\n",
    "yval = target.values[val_index]\n",
    "\n",
    "# use xgb.XGBRegressor as the model\n",
    "xgbmodel = xgb.XGBRegressor()\n",
    "\n",
    "# define the hyperparameter space\n",
    "param_dist = {\n",
    "                'n_estimators': [100,200,300,400,500,1000],\n",
    "                'learning_rate': [0.01,0.05,0.1,0.2,0.3],\n",
    "                'max_depth': [3,4,5,6,7,8,9,10],\n",
    "                'min_child_weight': [1,2,3,4,5,6,7,8,9,10],\n",
    "                'gamma': [0,0.1,0.2,0.3,0.4,0.5],\n",
    "                'subsample': [0.5,0.6,0.7,0.8,0.9,1],\n",
    "                'colsample_bytree': [0.5,0.6,0.7,0.8,0.9,1],\n",
    "                'reg_alpha': [0,0.1,0.2,0.3,0.4,0.5],\n",
    "                'reg_lambda': [0,0.1,0.2,0.3,0.4,0.5]\n",
    "                }\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 100\n",
    "random_search = RandomizedSearchCV(xgbmodel, param_distributions=param_dist,\n",
    "                                      n_iter=n_iter_search, cv=3, verbose=0, n_jobs=-1)\n",
    "\n",
    "random_search.fit(xtrain_sampled, ytrain_sampled)\n",
    "\n",
    "# get the best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "xgbmodel = xgb.XGBRegressor(**best_model.get_params())\n",
    "\n",
    "xgbmodel.fit(xtrain, ytrain)\n",
    "\n",
    "pred_val = xgbmodel.predict(xval)\n",
    "\n",
    "# clip pred_val to larger than 0\n",
    "pred_val = np.clip(pred_val, a_min=0, a_max=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSE on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print('xgb val rmse: ', np.sqrt(mean_squared_error(yval, pred_val)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
